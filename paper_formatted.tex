%! Author = JustinMa
%! Date = 5/22/25

\documentclass[12pt]{article}

% Packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[labelfont=it, textfont=it]{caption}
\usepackage{float}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{times}
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}

\doublespacing

% TikZ libraries and styles
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, shadows}

\tikzset{
    layer/.style={
        rectangle,
        draw=black!80,
        rounded corners,
        minimum width=5cm,
        minimum height=0.6cm,
        text centered,
        text width=7cm,
        font=\small,
        drop shadow,
        fill=#1
    },
    arrow/.style={
        -{Stealth[length=2mm]},
        thick,
        shorten >=2pt,
        shorten <=2pt
    }
}

\title{Investigating Overfitting in Convolutional Neural Networks}\date{}

\begin{document}

    \maketitle

    \section*{1. Introduction}

    Neural networks are a form of machine learning models, roughly inspired by how the human brain
    processes information. They have become increasingly prevalent in our developing world, providing
    powerful and unique solutions to a wide variety of problems. However, with power also comes risk,
    as training a neural network too much on a small dataset can cause overfitting. Overfitting occurs
    when a model learns the training data too well by essentially memorizing it, causing it to perform
    well on data in its training set but poorly on test data (data that were not in the training set).
    While it might seem that simply increasing a model’s size would always boost accuracy by allowing it to
    capture more patterns, extra parameters often give the network enough capacity to store the training set
    outright when the training set is small, allowing it to overfit. Smaller networks, by contrast, are forced to find the underlying patterns
    in the data because of their limited size. In this study, we investigate whether increasing the number of
    parameters in a convolutional neural network leads to greater overfitting when trained on a small dataset, as
    measured by the difference between its accuracy on the training dataset and the test dataset.

    \section*{2. Statistical Question}

    Does increasing the number of parameters in a fixed CNN architecture cause a greater difference
    between training and testing accuracy?

    \noindent\textbf{Hypotheses:}\newline
    \centerline{$H_0: \beta = 0$} \newline
    \centerline{$H_a: \beta > 0$} \newline
    Where $\beta$ is the true slope of the population least‐squares regression line that relates number
    of parameters of the model to the difference in accuracy of the model on the train dataset and the
    test dataset (train - test).

    \section*{3. Data Collection}

    We trained $100$ convolutional models each on the same randomly selected small subset of 100 images
    from the Canadian Institute For Advanced Research - 10 dataset (CIFAR-10), which consists of 32x32 color images labeled with one one of ten classes (e.g., plane, boat, etc).
    Each model had the same architecture, the only difference between them being the number of parameters (\textit{see Figure 1 below}).
    The training dataset was constructed by randomly selecting 10 of each class of image to ensure that it is representative of the entire dataset.
    The number of filters and neurons in each model were varied so that the model sizes are roughly uniform, ranging from approximately 1m - 50m parameters.
    The architecture and the ratio of layer sizes were also kept the same for each model, making sure the only difference between them is the number of parameters.
    On the initialization of each model in each set, the values of the parameters are randomly set, so each model can be seen as randomly
    selected from all possible models of their respective size and architecture before training, ensuring the experiment is statistically valid.
    We controlled the parameter count by multiplying the amount of filters for the Conv2D layers and the amount of neurons
    for the hidden Dense layers by a scalar factor, $n$.

    \noindent\textbf{Model Architecture}
    \begin{figure}[H]
        \begin{center}
            \begin{tikzpicture}[node distance=4mm and 0mm]
                \node[layer=gray!10]                     (input)  {Input $(32\times32\times3)$};
                \node[layer=blue!10,  below=of input]    (conv1)  {Conv2D: $32\cdot n$ filters, $3\times3$, ReLU};
                \node[layer=cyan!10,  below=of conv1]    (pool1)  {MaxPool: $2\times2$};
                \node[layer=blue!10,  below=of pool1]    (conv2)  {Conv2D: $64\cdot n$ filters, $3\times3$, ReLU};
                \node[layer=cyan!10,  below=of conv2]    (pool2)  {MaxPool: $2\times2$};
                \node[layer=orange!10,below=of pool2]    (flat)   {Flatten};
                \node[layer=green!15, below=of flat]    (dense1) {Dense: $128\cdot n$ neurons, ReLU};
                \node[layer=green!15, below=of dense1]  (dense2) {Dense: $64\cdot n$ neurons, ReLU};
                \node[layer=red!15,   below=of dense2]  (output) {Dense (Output): 10, Softmax};

                \draw[arrow] (input)  -- (conv1);
                \draw[arrow] (conv1)  -- (pool1);
                \draw[arrow] (pool1)  -- (conv2);
                \draw[arrow] (conv2)  -- (pool2);
                \draw[arrow] (pool2)  -- (flat);
                \draw[arrow] (flat)   -- (dense1);
                \draw[arrow] (dense1) -- (dense2);
                \draw[arrow] (dense2) -- (output);
            \end{tikzpicture}
            \caption{General Architecture of Our Models}
        \end{center}
        \label{fig:architecture}
    \end{figure}

    Each model was trained for $100$ epochs on the randomly selected subset of 100 images and then tested
    on the full test set of the CIFAR-10 dataset, made up of $10,000$ images. The difference between the train and test accuracy (train - test) was then
    computed for each model and graphed on a scatter plot with the x-axis
    as parameter count and the y-axis as the difference between train and test accuracy for each model.

    \section*{4. Data Display}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{Images/Scatter}
        \caption{Scatterplot of Parameter Count vs. Difference in Train and Test Accuracy (Train - Test)}
        \label{fig:scatterplot}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{Images/Resid}
        \caption{Residual Plot (Parameter Count vs. Residuals)}
        \label{fig:residuals}
    \end{figure}

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{Images/Resid Hist}
        \caption{Histogram of Residuals}
        \label{fig:residual_hist}
    \end{figure}

    \section*{5. Data Analysis}

    We conducted a $t$-test for the population slope $\beta$ of the least-squares regression line that relates the number of parameters in a model trained on a 100-image subset of the CIFAR-10 dataset to the difference in accuracy between training and test data (Train Accuracy $-$ Test Accuracy).

    \subsection*{5.1 Condition Check}

    \begin{quote}

    \textbf{5.1.1 Linear}

    The scatterplot of the data (\textit{\autoref{fig:scatterplot}}) shows a clear linear trend, and the residual plot (\textit{\autoref{fig:residuals}}) shows no remaining curved pattern.
    Therefore, the linearity condition is satisfied.

    \textbf{5.1.2 Independent}

    Each model was trained independently with no interaction between runs. Thus, the independence condition is met by the design of the experiment.

    \textbf{5.1.3 Normal}

    The histogram of the residuals (\textit{\autoref{fig:residual_hist}}) shows no strong skew and no obvious outliers,
    so the normality condition is met.

    \textbf{5.1.4 Equal Variance}

    The residual plot (\textit{\autoref{fig:residuals}}) shows shows no noticeable $<$ or $>$ shape, so the equal variance condition is satisfied.

    \textbf{5.1.5 Random}

    The parameters of the model were randomized on initialization, so each model can be seen as randomly selected from the population
    of models of the same size and structure.

    \end{quote}

    \subsection*{5.2 Calculations}

    We used the following equation to calculate the Standard Error $SE_\beta$ of the slope \beta:

    \begin{equation*}
        \centering
        SE_\beta = \frac{s}{s_x\sqrt{n - 1}}
    \end{equation*}

    \section*{6. Conclusion}

    \section*{7. Reflection}

\end{document}
